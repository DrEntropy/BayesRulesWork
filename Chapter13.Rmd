# Chapter 13 - Hotel Bookings

For this chapter, since I did some logistic regression in Python for Kaggle, lets try Rstan !

```{r}
# Load packages
library(bayesrules)
library(rstanarm)
library(bayesplot)
library(tidyverse)
library(tidybayes)
library(broom.mixed)
```


```{r}
data("hotel_bookings")
```

## Exercise 13.6 

### Proportion of the sample bookings were cancelled?
 %
```{r}
hotel_bookings %>%  
   select(is_canceled)  %>% summarize(fraction_cancelled = mean(is_canceled==1))
```


### Look at plots of each predictor vs `is_canceled`

The predictors the problem are `lead time` `previous_cancellations` `is_repeated_guest` and `average_daily_rate`

```{r}
hotel_bookings <- hotel_bookings %>% 
   select(is_canceled,lead_time, previous_cancellations, is_repeated_guest, average_daily_rate )
```
 
Average valuesa  and max values are of interest:

```{r}
hotel_bookings %>% summarise(across(where(is.numeric), mean))
```
```{r}
hotel_bookings %>% summarise(across(where(is.numeric), max))
```


```{r}
hotel_bookings %>% group_by(is_repeated_guest) %>% 
  summarize(cancel_rate = mean(is_canceled==1))
```

Looks like repeated guests are less likely to cancel (makes sense)

For the others, we will need to make some cuts to bin the quantitative variables:

```{r}
hotel_bookings %>%
  mutate(lead_time_cut = cut(lead_time, breaks =10)) %>%
   group_by(lead_time_cut) %>% summarize(cancel_rate = mean(is_canceled==1)) %>%
  ggplot(aes(x= lead_time_cut, y= cancel_rate)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle=45, vjust =0.5))
```
 
Longer lead times lead to more cancellations
 
```{r}
hotel_bookings %>%
  mutate(previous_cancel_cut = cut(previous_cancellations, breaks = c(-5,0,5,10,15,25,30))) %>%
   group_by(previous_cancel_cut) %>% summarize(cancel_rate = mean(is_canceled==1)) %>%
  ggplot(aes(x= previous_cancel_cut, y= cancel_rate)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle=45, vjust =0.5))
```
 
Previously cancelling many times is a very strong predictor.



```{r}
hotel_bookings %>%
  mutate(daily_rate_cut = cut(average_daily_rate, breaks = seq(-50,400,50))) %>%
   group_by(daily_rate_cut) %>% summarize(cancel_rate = mean(is_canceled==1)) %>%
  ggplot(aes(x= daily_rate_cut, y= cancel_rate)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle=45, vjust =0.5))
```

Weak predictor, but higher rates lead to more cancellations (except for the very high!)
Note the odd 'free' hotels. 

```{r}
hotel_bookings %>%
  mutate(daily_rate_cut = cut(average_daily_rate, breaks = seq(-50,400,50))) %>%
   group_by(daily_rate_cut) %>% count

``` 

### Model choice

For the model we will use  `is_cancelled ~ lead_time + previous_cancellations + is_repeated_guest + average_daily_rate` with Binomial family and default logit link.

I will use weakly informative priors centered at zero, including for the centered intercept.


## Exercise 13.7 

### Simulation of posterior
                             
```{r}
cancel_model <- stan_glm(is_canceled ~ lead_time + previous_cancellations + is_repeated_guest + average_daily_rate,
                             data = hotel_bookings, family = binomial,
                             prior_intercept = normal(0,1,autoscale=TRUE),
                             prior = normal(0,1, autoscale=TRUE),
                             chains = 4, iter = 5000*2, seed = 84735 )
```


```{r}
prior_summary(cancel_model)
```
```{r}
mcmc_trace(cancel_model)
```

```{r}
mcmc_dens_overlay(cancel_model)
```


### Model check (PP check)

```{r}
proportion_cancel <- function(x){mean(x == 1)}

pp_check(cancel_model, nreps = 100,
         plotfun = "stat", stat = "proportion_cancel") + 
  xlab("probability of cancel")
```

Looks good. Now lets look at the model

### Parameter posteriors 


```{r}
tidy(cancel_model, effects = "fixed", conf.int = TRUE, conf.level = 0.80)
```

The model is simply the logit of the linear model described above, not going to write it down though.
 
Note that these are log odds. to get odds;

```{r}
exp(posterior_interval(cancel_model,prob=.8))
```




Looks to me that all except perhaps `is_repeated_guest` are statistically significant.

For lead time , each 10 days of lead time will cause about a 5% increase in odds of cancelling , so is not very strong
For previous cancellations, it is clear that having multiple previous cancellations is storng.
For repeated_guest,  being repeat guest means the odds are decreased by about half. 
Finally every 10 dollar increase in rate causes an increats of about 7% as well in the odds, so not very strong there either.




## Exercise 13.8

### Accuracy

First using the `bayesrules` shortcut:

```{r}
classification_summary(model = cancel_model, data= hotel_bookings, cutoff = 0.5)$confusion_matrix
```

Lets do it by hand:

```{r}
cancel_pred <- posterior_predict(cancel_model)
dim(cancel_pred)
```

`cancel_pred` contains 20000 predictions for each data point in the original data.  We now compute the p value fore ach one


```{r}
cancel_classifications <- hotel_bookings %>% 
  mutate(cancel_prob = colMeans(cancel_pred),
         cancel_class = as.numeric(cancel_prob >= 0.5),
         match = cancel_class == is_canceled) 
```


```{r}
cancel_classifications  %>% 
  group_by(cancel_class,is_canceled) %>% count %>% pivot_wider(names_from=is_canceled,values_from=n)
```

This gives the confusion matrix using the best of my abilities in R at the moment ;).  The book uses `table`, which is perhaps easier.

Overall prediction accuracy with 50% cut off:

```{r}
mean(cancel_classifications$match)
```

68% accuracy in sample

Cross validation:

```{r}
cv_accuracy <- classification_summary_cv(
  model = cancel_model, data = hotel_bookings, cutoff = 0.5, k = 10)
```

```{r}
cv_accuracy$cv
```

Accuracy basically matches, so we are not overfitting by this metric.

### Achieve 75% sensitivity.

We would like a sensitivity of 75%.  What cutoff will do that, and what is the specificity?  By trial and error I find 0.3 works well enough:

```{r}
classification_summary(model = cancel_model, data= hotel_bookings, cutoff = 0.3)$accuracy_rates
```

A cut off of 0.3 gives us 77% (0.31 drops that to 72%) so lets stick with this. With this cutt off we identify true cancellations about 77% of the time, but also falsly identify cancelations about half the time 51% which is significantly worse then it was before.  If it were me I would want a false alarm rate below 10% to be sure not to piss off customers, which is what we get with a cutoff of about 0.5. But a true cost of false alarms vs cost of cancelled bookings would need to be considered.



Check cv:

```{r}
cv_accuracy <- classification_summary_cv(
  model = cancel_model, data = hotel_bookings, cutoff = 0.3, k = 10)
```

```{r}
cv_accuracy$cv
```

## Exercise 13.9

A guest that is new to a hotel and has only canceled a booking 1 time before, has booked a $100 per day hotel room 30 days in advance. Simulate, plot, and discuss the posterior predictive model of  Y, whether or not the guest will cancel this booking.

```{r}
cancel_guest <- posterior_predict(cancel_model, newdata = data.frame(previous_cancellations = c(1), 
                                                                     is_repeated_guest=c(0), average_daily_rate=c(100),
                                                                     lead_time= c(30)))
dim(cancel_guest)
mean(cancel_guest)
```

76% chance that this guest will cancel.

We can do this directly with the model rather then posterior predict:
```{r}
customer_model <- as.data.frame(cancel_model) %>% 
  mutate(log_odds = `(Intercept)` + previous_cancellations*1 + average_daily_rate*100 + lead_time*30,
         odds = exp(log_odds),
         prob = odds / (1 + odds),
         Y = rbinom(20000, size = 1, prob = prob))
```

```{r}
mean(customer_model$prob)
```

Or using Y
```{r}
mean(customer_model$Y)
```

Note that $\bar{Y} = \Sigma p P(p)  = \bar{p}$  so we expect these to be the same within statistical accuracy.

If we wanted someone that was even MORE likely to cancel the key thing is lots of previous cancellations, lets say 10 previous cancellations. Lets assume it is a repeated guest to make it more realistic

```{r}
customer_model2 <- as.data.frame(cancel_model) %>% 
  mutate(log_odds = `(Intercept)` + previous_cancellations*10+ is_repeated_guest + average_daily_rate*100 + lead_time*30,
         odds = exp(log_odds),
         prob = odds / (1 + odds),
         Y = rbinom(20000, size = 1, prob = prob))
mean(customer_model2$Y)
mean(customer_model2$prob)
```

CERTAIN to cancel. 



